\documentclass[../body.tex]{subfiles}
\begin{document}
	\vspace{\baselineskip}
	SVM – отличный метод. Но и у него есть недостатки.
	\begin{itemize}
		\item  На выходы у SVM мы имеем решение, а апостериорные вероятности, которые в некоторых задачах оказываются более значимыми получить из чистого SVM не предоставляется возможным
		\item  SVM используется для двух классов, обобщить на несколько
		проблематично.
		\item Есть параметр $с$ (или $\nu$, или ещё вдобавок $\epsilon$), подбор которых сопровождается рядом проблем
		\item Предсказания – линейные комбинации ядер, которым
		необходимо быть положительно определёнными и которые
		центрированы на точках из датасета.
	\end{itemize}
	Поэтому существует огромное количество модификаций SVM алгоритма, которые стараются решить недостатки SVM 
	\\
	
	На основе анализа мировой литературы будем рассматривать четыре критерия для
	оценивания «качества» селективных свойств конкретного метода обучения:
	\begin{enumerate}
		\item эффективное подавление полностью шумовых попарно независимых признаков;
		\item эффективное подавление полностью шумовых признаков, имеющих значительную
		линейную зависимость между собой;
		\item  эффективное выделение группы информативных линейно независимых признаков;
		\item эффективное выделение группы информативных признаков, только совместное участие которых в решении может обеспечить приемлемую точность распознавания.
	\end{enumerate}.
	По этим критериям рассмотрим две наиболее популярных модификации метода опорных векторов, наделяющие его свойством отбора признаков - \textbf{Lasso SVM
		(1-norm SVM) и Elastic Net SVM (Doubly Regularized SVM)}.
	
	\vspace{\baselineskip}
	Оба эти метода несколько разными средствами отбирают подмножество информативных признаков, число которых определяется структурными параметрами.
	
	
	Рассмотрим критерий, который представляет собой обобщение классического метода опорных векторов
	Критерий обучения эквивалентен оптимизационной задаче минимизации целевой функции $J(a, b, \delta |c)$ на выпуклом множестве, заданном набором линейных ограничений-неравенств для объектов обучающей совокупности: 
	$$\left\{\begin{matrix}
		J(a, b, \delta |c) = -\ln{\varPsi(a)}+ \sum_{j=1}^{N}(\delta_j)->min(a,b,\delta)
		\\
		y_i(\sum_{i=1}^{n}(a_ix_{ij})+b)\geq1-\delta_j, \delta_j \geq 0, j= 1,...,N
	\end{matrix}\right.$$
	
	\subsection{1-norm SVM (LASSO SVM) и Doubly Regularized SVM (ElasticNet SVM)}
	
	Известные методы автоматического сокращения размерности признакового описания объектов в рамках метода опорных векторов, а именно, 1-погт SVM (Lasso
	SVM)" и Doubly Regularized SVM (Elastic Net SVM) получаются из обобщенного критерия (13) специальным выбором априорной плотности распределения направляющего вектора =$\varPsi(a)$. Метод Elastic Net SVM, как более общий из них, получается из (13) при предположении, что случайный направляющий вектор образован независимыми компонентами $a=(\alpha_1, ..., \alpha_n)$
	с одинаковыми плотностями распределения 
	\[\varPsi(\alpha_i|\beta,\mu)= Dexp[-(\beta\alpha_i^2+\mu|\alpha_i|)]\]
	которые определяются одинаковыми значениями параметров $(\beta, \mu)$. Здесь нормирующая константа определяется обоими параметрами:
	$$D=[2\sqrt{\frac{\pi}{\beta}}exp(\frac{\mu^2}{4\beta})\Phi(\dfrac{\mu}{\sqrt{2\beta}})]\mbox{где }\Phi(u)=\frac{1}{2\pi}(\int_{u}^{\infty}exp(-\frac{z^2}{2})\,dz\)\mbox{ - это функция Лапласса}$$
	В этом случае совместная априорная плотность распределения направляющего вектора определяется выражением $$\varPsi(\alpha_i|\beta,\mu)=D^{n}exp(-\sum_{i=1}^{n}(\beta*\alpha^{2}_{i}+\mu|\alpha_{i}|))$$
	
	\begin{equation}
		\ln\varPsi(\alpha_i|\beta,\mu) = const - (\beta\sum_{i=1}^{n}(\alpha_i^2)+\mu\sum_{i=1}^{n}(|\alpha_i|))
	\end{equation}
	,и (13) примет вид, эквивалентный методу Elastic Net SVM: 
	$$\left\{\begin{matrix}
		J_{DrSVM}(a, b, \delta |c,\beta ,\mu ) = \beta\sum_{i=1}^{n}(\alpha_i^2)+\mu\sum_{i=1}^{n}(|\alpha_i|) + c\sum_{j=1}^{N}(\delta_j)->min(a,b,\delta)
		\\
		y_i(\sum_{i=1}^{n}(a_ix_{ij})+b)\geq1-\delta_j, \delta_j \geq 0, j= 1,...,N
	\end{matrix}\right.$$
	В еще более частном случае, когда $\beta = 0$, получается метод обучения Lasso SVM:
	$$\left\{\begin{matrix}
		J_{1nSVM}(a, b, \delta |c,r ) = \sum_{i=1}^{n}|\alpha_i| + c(r/2)^{\frac{1}{2}}\sum_{j=1}^{N}(\delta_j)->min(a,b,\delta)
		\\
		y_i(\sum_{i=1}^{n}(a_ix_{ij})+b)\geq1-\delta_j, \delta_j \geq 0, j= 1,...,N
	\end{matrix}\right.$$
	
	Этот критерий эквивалентен (13) при интерпретации независимых компонент случайного
	направляющего вектора $a=(\alpha_1, ..., \alpha_n)$ как распределенных по закону Лапласа $\psi(\alpha_i|r) = (2r)^{-\frac{1}{2}}|\alpha_i|$
	В силу специфики принятых априорных распределений критерии (16) и (17) чрезвычайно важным свойством беспереборного отбора признаков непосредственно в ходе обучения. Это свойство является результатом
	склонности этих критериев в точке минимума присваивать нулевые значения большинству
	компонент направляющего вектора $\alpha_i = 0$, реализуя при этом, вообще говоря, разумный
	отбор подмножества «полезных» признаков. 
	
	Суждение о классическом SVM тривиально - все направления одинаковы по предпочтительности, и свойство селективности полностью отсутствует. Что же касается методов
	Elastic Net SVM и Lasso SVM, то оказалось, что они идентичны по ориентации предпочтительных направлений - наилучшим является направление, оставляющее единственный
	признак, а роль наихудшего ифает направление, оценивающее все признаки как равнозначные. Это означает, что соответствующие априорные распределения направляющего
	вектора выражают предположение, что среди признаков есть только один признак, полезный для распознавания класса объекта. Следовательно, в ситуациях, когда в исходных
	данных таких признаков более одного, обучение методами Elastic Net SVM и Lasso SVM
	может приводить к снижению обобщающей способности полученных решающих функций.
	Известно также''', что Lasso SVM в группе линейно зависимых признаков полностью
	игнорирует факт их зависимости - отбирает лишь один такой признак, отбрасывая остальные. В то же время. Elastic Net SVM стремится оставлять всю линейно зависимую группу
	целиком, даже если она малоинформативна. В диссертации показано, что последнее свойство Elastic Net SVM также ведет к снижению обобшающей способности обучения.
	Таким образом, необходима разработка новых способов регуляризации SVM, способных, во-первых, допускать существование независимых одинаково полезных признаков, и,
	во-вторых, удалять шумовые признаки без учета их линейной зависимости. 
	\\
	
	Третья проблемная ситуация определяется тем, что, как оказалось, эти методы далеко не удовлетворяют сочетаниям пар требований (а-б) и (в-г).
	Для разрешения этой проблемной ситуации в диссертации разработаны два новых
	класса априорных распределений направляющего вектора дискримииантной гиперплоскости и, следовательно, две новые модификации метода опорных векторов.
	Один из новых методов, названный .«ewoôo.M релевантных признаков {Relevance Feature Machine - RFM), не выделяя строгого подмножества информативных нрггзпаков. наделяет все признаки неотрицательными весами. Чем больше значе[1ие сгруктурного параметра селективности, тем большее число весов приближаются к нулевым значениям, фактически исключая соответствующие признаки из принятия решения о классе объекта.
		Другой предложенный метод, названный методом опорных признаков (Support Feature
		Machine - SFM), разбивает все множество признаков на три группы - полиостью активные
		признаки, взвешенные признаки и полностью подавленные признаки. Можно считать, что
		метод SFM снабжает все признаки весами, но, в отличие от метода RFM, часть весов оказываются строгими единицами, часть принимает значения между единицей и нулем, а
		часть строго равны нулю
		\subsection{Метод релевантных признаков с регулируемой селективностью }
		В дополнение к уже рассмотренным методам регуляризации,
		предложены два новых класса априорных распределений направляющего вектора, наделяющих обобщенный критерий обучения (13) свойством автоматического отбора признаков.
		Метод релевантных признаков с регулируемой селективностью использует естественное обобщение классического метода опорных векторов за счет введения в его вероятностную постановку дополнительного предположения о том, что независимые компоненты направляющего вектора $a=(\alpha_1, ..., \alpha_n)$ распределены, как и прежде, по нормальному закону с нулевыми математическими ожиданиями, но с разными неизвестными случайными дисперсиями $r_i \geq 0, i = 1,...,n$ характеризующимися некоторым априорным распределением, и подлежащими оцениванию в процессе обучения по байесовскому принципу вместе с параметрами разделяющей гиперплоскости $(a,b)$. 
		В данном методе оптимизациояная задача обучения эквивалентна
		критерию:
		$$\left\{\begin{matrix}
			J_{RFM}(a, b, r, \delta |C,\mu) = \sum_{i=1}^{n}[(\frac{1}{r_i})(\alpha_i^2 +(\frac{1}{\mu}))+((\frac{1}{\mu})+1+\mu)\ln{r_i}] + C\sum_{j=1}^{N}(\delta_j)->min(a,b,r,\delta)
			\\
			y_i(\sum_{i=1}^{n}(a_ix_{ij})+b)\geq1-\delta_j, \delta_j \geq 0, j= 1,...,N, C=2c
		\end{matrix}\right.$$
		
		Итерационная процедура обучения обычно сходится за 10-15 шагов, а сам алгоритм
		демонстрирует способность подавлять неинформативные признаки за счет выбора очень
		маленьких весов в решающем правиле.
		\\
		Идея погружения дискретной задачи выбора подмножества в непрерывную задачу поиска в некотором смысле оптимальных неотрицательных весов, присваиваемых элементам исходного множества, заимствована из метода релевантных векторов Бишопа и Типпинга' , в
		котором направляющий вектор разделяющей гиперплоскости строится как взвешенная линейная комбинация векторов признаков всех объектов в обучающей совокупности. Векторы
		признаков объектов, получившие существенно ненулевые веса, названы Бишопом и Типпингом релевантными векторами в отличие от опорных векторов в методе опорных векторов, образующих жестко выделенное подмножество. В отличие от этот прием использован в диссертации для «мягкого» отбора признаков, а не объектов обучающей совокупности,
		участвующих в итоговом решающем правиле распознавания. Главным же отличием метода,
		предлагаемого в диссертации, является принципиально новое понятие параметра селективности ц .
		В связи с этим обстоятельством предложенный в диссертации метод обучения (22)
		назван методом релевантных признаков с регулируемой селективностью, или, полностью
		характеризуя метод. Selective Relevance Feature Support Vector Machine.
		\subsection{Метод опорных признаков с регулируемой селективностью}
		Данный метод базируется на другом
		предположении об априорном распределении $\varPsi(a)$ независимых компонент направляющего вектора а $a=(\alpha_1, ..., \alpha_n)$. Вид априорной плотности $\varPsi(a)$ представляет собой комбинацию распределения Лапласа при значениях нормы компонент, не превышающих заданного
		порога I а,|< ц, и нормального распределения при больших значениях $|\alpha_{i}>\geq\mu|$ 
		В данном методе мы имеем следующий вид оптимизационной задачи обучения:
		$$\left\{\begin{matrix}
			J_{SFM}(a, b, \delta |C,\mu) = 2\mu\sum_{|\alpha_{i}|\leq\mu}|\alpha_{i}| + \sum_{|\alpha_{i}|\leq\mu}(\alpha_{i}^2 + \mu^2)+ C\sum_{j=1}^{N}(\delta_j)->min(a,b,\delta)
			\\
			y_i(\sum_{i=1}^{n}(a_ix_{ij})+b)\geq1-\delta_j, \delta_j \geq 0, j= 1,...,N, C=2c
		\end{matrix}\right.$$
		Задача является задачей выпуклого программирования в пространстве $R^n \times R^{N+1}$, поскольку каждое слагаемое целевой функций выпукло в $R^n$, а система ограничений для заданной обучающей выборки образует выпуклый многогранник.
		
		Оптимальные ненулевые значения компонент направляющего вектора, как и в классическом методе опорных векторов, выражаются в виде линейной комбинации опорных объектов обучающей выборки, но, в отличие от SVM, решение задачи явным образом указывает множество строго нулевых компонент, а, следовательно, множество неактивных или неопорных признаков, т.е. признаков, не участвующих
		в принятии решения о классе нового объекта. По этой причине, по аналогии с методом
		опорных векторов, предложенный метод назван  методом опорных признаков
		с регулируемой селективностью или Selective Support Feature Support Vector Machine. 
		
		\subparagraph{Сравнение методов}
		Преимущества предложенных двух критериев селективного обучения метода релевантных признаков (22) и метода опорных признаков (27) по сравнению с известными критериями Elastic Net SVM (16) и Lasso SVM (17) исследуются в диссертации с теоретической точки зрения в конце главы 3 и экспериментально в главе 4.
		В частности, теоретическое исследование регуляризующей способности метода опорных признаков (27) показало, что множество наименее предпочтительных ориентаций
		направляющего вектора состоит не из одного направления, оценивающего все признаки
		как равнозначные (методы Elastic Net SVM и Lasso SVM), a составляет конус направлений
		{а е R" : 1'=1.-.'' } > предполагающих включение в решающую функцию
		+ сразу всех признаков с разными достаточно большими весами |а, |>ец, г=1,...,и.
		Это условие означает «неразличимость» для обучения таких направляющих векторов между собой с точки зрения их априорной предпочтительности. Следовательно, в ипуациях,
		когда в исходных данных есть несколько значимых признаков, выбор весов, с которыми
		они будут входить в решение, определяется исключительно качеством разделения выборки, а не априорным стремлением выделить только один из признаков, подобно методам
		Elastic Net SVM и Lasso SVM.
		С другой стороны, предложенный метод опорных признаков (27) характеризуется избирательным «стилем» учета линейной зависимости признаковых описаний объектов при
		их отборе. Избирательность предложенного метода выражается в стремлении сохранять в
		итоговом решении только попарно зависимые признаки, обладающие достаточной значимостью для распознавания. Для сравнения метод Elastic Net SVM сохраняет все линейно
		зависимые признаки. Малозначимые признаки при этом максимально подавляются в процессе обучения независимо от их попарной линейной зависимости, аналогично Lasso SVM.
		\\
		приводятся результаты экспериментального исследования предложенных в диссертации методов обучения с регулируемой селективностью, а именно, метода релевантных признаков (22) и метода опорных признаков (27). Основной целью экспериментального исследования является анализ предложенных методов в сравнении с существующими методами SVM (5), Lasso SVM (17) и Elastic Net SVM (16) по их способности
		сокращать признаковое описание объектов распознавания и, в конечном итоге, повышать
		обобщающую способность обучения при относительно малой обучающей выборке и
		большом числе признаков.
		Экспериментальное исследование имеет общепринятую структуру, включающую в себя серию модельных экспериментов и пример решения прикладной задачи. Модельные
		эксперименты диссертации основаны, по своей структуре, на исследовании, проведенном
		авторами метода Elastic Net SYM'"* для иллюстрации преимущества модульноквадратичной функции штрафа Elastic Net по сравнению с традиционным модульным Lasso. Прикладной задачей является задача распознавания рака легких из репозитория UCl'®.
		С одной стороны, преимущество модельных экспериментов заключается в том, что они
		дают возможность придать условным понятиям полезных и лишних признаков абсолютный смысл, и позволяют для каждой конкретной гиперплоскости непосредственно вычислить вероятность ошибки распознавания на генеральной совокупности. С другой стороны,
		это позволяет сравнивать методы в заведомо разных условиях порождения исходных данных, скрытых для наблюдения в прикладных задачах и выраженных в знании истинного
		положения искомой гиперплоскости и особенностях распределений признакового описания объектов.
		Именно этот аспект организации экспериментального исследования заслуживает особого внимания. Структура четырех модельных задач настоящей диссертации соответствует
		четырем простым требованиям к селективному обучению, приведенным на с. 5 автореферата. Результаты модельных экспериментов, изложенные в диссертации, наглядно иллюстрируют весьма важный факт, что ни один из существующих методов селективного обучения, включая предлагаемые, не удовлетворяет сразу всем этим весьма неизощренным
		требованиям. Предложенный метод опорных признаков (27) хорошо справился с подавлением шумовых признаков (а,б) при линейно независимых информативных признаках (в),
		существенно улучшив и без того неплохой результат существующего метода Elastic Net
		SVM. Однако метод релевантных признаков (22) в этих условиях оказался далеко не так
		эффективен. Требование выделять группу информативных признаков, которые только
		вместе обеспечивают достаточную точность распознавания (г), оказалось весьма проблематичным, как для существующего Elastic Net SVM, так и для предложенного метода
		опорных признаков, селективность которых обеспечивается за счет использования в целевых функциях критериев обучения штрафа модуля. Вместе с тем требование (г) не доставило никаких сложностей для второго предложенного метода релевантных признаков, селективность которого имеет отличную от привычного модуля природу.
		Полученные в рамках диссертационной работы результаты экспериментального исследования иллюстрируют полезность предложенных методов обучения в сравнении с известными образцами. 
		
	\end{document}
	
	
