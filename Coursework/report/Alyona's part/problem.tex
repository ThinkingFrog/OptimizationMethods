\documentclass[../body.tex]{subfiles}
\begin{document}
\vspace{\baselineskip}
SVM – отличный метод. Но и у него есть недостатки.
\begin{itemize}
	\item  На выходы у SVM мы имеем решение, а апостериорные вероятности, которые в некоторых задачах оказываются более значимыми получить из чистого SVM не предоставляется возможным
	\item  SVM используется для двух классов, обобщить на несколько
	проблематично.
	\item Есть параметр $с$ (или $\nu$, или ещё вдобавок $\epsilon$), подбор которых сопровождается рядом проблем
	\item Предсказания – линейные комбинации ядер, которым
	необходимо быть положительно определёнными и которые
	центрированы на точках из датасета.
\end{itemize}
Поэтому существует огромное количество модификаций SVM алгоритма, которые стараются решить недостатки SVM 
\\

На основе анализа мировой литературы будем рассматривать четыре критерия для
оценивания «качества» селективных свойств конкретного метода обучения[3,5]:
\begin{enumerate}\label{points}
	\item эффективное подавление полностью шумовых попарно независимых признаков;
	\item эффективное подавление полностью шумовых признаков, имеющих значительную
	линейную зависимость между собой;
	\item  эффективное выделение группы информативных линейно независимых признаков;
	\item эффективное выделение группы информативных признаков, только совместное участие которых в решении может обеспечить приемлемую точность распознавания.
\end{enumerate}.

Рассмотрим критерий, который представляет собой обобщение классического метода опорных векторов.
Критерий обучения эквивалентен оптимизационной задаче минимизации целевой функции $J(a, b, \delta |c)$ на выпуклом множестве, заданном набором линейных ограничений-неравенств для объектов обучающей совокупности [3,11]: 
\begin{equation}\label{criterion}
	\left\{\begin{matrix}
		J(a, b, \delta |c) = -\ln{\varPsi(a)}+ \sum_{j=1}^{N}(\delta_j)->min(a,b,\delta)
		\\
		y_i(\sum_{i=1}^{n}(a_ix_{ij})+b)\geq1-\delta_j, \delta_j \geq 0, j= 1,...,N 
	\end{matrix}\right.
\end{equation}
Наиболее популярными модификации метода опорных векторов, наделяющие его свойством отбора признаков являются \textbf{Lasso SVM
	(1-norm SVM) и Elastic Net SVM (Doubly Regularized SVM)}.

\vspace{\baselineskip}
Оба эти метода несколько разными средствами отбирают подмножество информативных признаков, число которых определяется структурными параметрами.
\subsection{1-norm SVM (LASSO SVM) и Doubly Regularized SVM (ElasticNet SVM)}

Известные методы автоматического сокращения размерности признакового описания объектов в рамках метода опорных векторов,  получаются из обобщенного критерия (\ref{criterion}) специальным выбором априорной плотности распределения направляющего вектора =$\varPsi(a)$. Метод Elastic Net SVM, как более общий из них, получается из (\ref{criterion})  при предположении, что случайный направляющий вектор образован независимыми компонентами $a=(\alpha_1, ..., \alpha_n)$
с одинаковыми плотностями распределения 
\[\varPsi(\alpha_i|\beta,\mu)= Dexp[-(\beta\alpha_i^2+\mu|\alpha_i|)]\]
которые определяются одинаковыми значениями параметров $(\beta, \mu)$. Здесь нормирующая константа определяется обоими параметрами:
$$D=[2\sqrt{\frac{\pi}{\beta}}exp(\frac{\mu^2}{4\beta})\Phi(\dfrac{\mu}{\sqrt{2\beta}})]\mbox{где }\Phi(u)=\frac{1}{2\pi}(\int_{u}^{\infty}exp(-\frac{z^2}{2})dz)\mbox{ - это функция Лапласса}$$
В этом случае совместная априорная плотность распределения направляющего вектора определяется выражением $$\varPsi(\alpha_i|\beta,\mu)=D^{n}exp(-\sum_{i=1}^{n}(\beta*\alpha^{2}_{i}+\mu|\alpha_{i}|))$$

$$
\ln\varPsi(\alpha_i|\beta,\mu) = const - (\beta\sum_{i=1}^{n}(\alpha_i^2)+\mu\sum_{i=1}^{n}(|\alpha_i|))
$$
 и (\ref{criterion}) примет вид, эквивалентный методу Elastic Net SVM [3,13]: 
$$\left\{\begin{matrix}
J_{DrSVM}(a, b, \delta |c,\beta ,\mu ) = \beta\sum_{i=1}^{n}(\alpha_i^2)+\mu\sum_{i=1}^{n}(|\alpha_i|) + c\sum_{j=1}^{N}(\delta_j)->min(a,b,\delta)
	\\
	y_i(\sum_{i=1}^{n}(a_ix_{ij})+b)\geq1-\delta_j, \delta_j \geq 0, j= 1,...,N
\end{matrix}\right.$$
В еще более частном случае, когда $\beta = 0$, получается метод обучения Lasso SVM [3,13]:
$$\left\{\begin{matrix}
	J_{1nSVM}(a, b, \delta |c,r ) = \sum_{i=1}^{n}|\alpha_i| + c(r/2)^{\frac{1}{2}}\sum_{j=1}^{N}(\delta_j)->min(a,b,\delta)
	\\
	y_i(\sum_{i=1}^{n}(a_ix_{ij})+b)\geq1-\delta_j, \delta_j \geq 0, j= 1,...,N
\end{matrix}\right.$$

Этот критерий эквивалентен (\ref{criterion}) при интерпретации независимых компонент случайного
направляющего вектора $a=(\alpha_1, ..., \alpha_n)$ как распределенных по закону Лапласа $\psi(\alpha_i|r) = (2r)^{-\frac{1}{2}}|\alpha_i|$
В силу специфики принятых априорных распределений чрезвычайно важным свойством беспереборного отбора признаков непосредственно в ходе обучения. Это свойство является результатом
склонности этих критериев в точке минимума присваивать нулевые значения большинству
компонент направляющего вектора $\alpha_i = 0$, реализуя при этом, вообще говоря, разумный
отбор подмножества «полезных» признаков. 
\\

Суждение о классическом SVM тривиально - все направления одинаковы по предпочтительности, и свойство селективности полностью отсутствует. Что же касается методов
Elastic Net SVM и Lasso SVM, то оказалось, что они идентичны по ориентации предпочтительных направлений - наилучшим является направление, оставляющее единственный
признак, а роль наихудшего играет направление, оценивающее все признаки как равнозначные. Это означает, что соответствующие априорные распределения направляющего
вектора выражают предположение, что среди признаков есть только один признак, полезный для распознавания класса объекта. Следовательно, в ситуациях, когда в исходных
данных таких признаков более одного, обучение методами Elastic Net SVM и Lasso SVM
может приводить к снижению обобщающей способности полученных решающих функций.
Известно также, что Lasso SVM в группе линейно зависимых признаков полностью
игнорирует факт их зависимости - отбирает лишь один такой признак, отбрасывая остальные. В то же время Elastic Net SVM стремится оставлять всю линейно зависимую группу
целиком, даже если она малоинформативна. 
Таким образом, необходима разработка новых способов регуляризации SVM, способных, во-первых, допускать существование независимых одинаково полезных признаков, и,
во-вторых, удалять шумовые признаки без учета их линейной зависимости. 
\\

Для разрешения этой проблемной ситуации разработаны два новых
класса априорных распределений направляющего вектора дискримииантной гиперплоскости и, следовательно, две новые модификации метода опорных векторов.
\subsection{Методы с регулируемой селективностью }
Один из новых методов, названный метод релевантных признаков (Relevance Feature Machine - RFM), не выделяя строгого подмножества информативных признаков, наделяет все признаки неотрицательными весами [3,14]. Чем больше значение структурного параметра селективности, тем большее число весов приближаются к нулевым значениям, фактически исключая соответствующие признаки из принятия решения о классе объекта.
	Другой предложенный метод, названный методом опорных признаков (Support Feature
	Machine - SFM), разбивает все множество признаков на три группы - полиостью активные
	признаки, взвешенные признаки и полностью подавленные признаки. Можно считать, что
	метод SFM снабжает все признаки весами, но, в отличие от метода RFM, часть весов оказываются строгими единицами, часть принимает значения между единицей и нулем, а
	часть строго равны нулю
\subsubsection{Метод релевантных признаков с регулируемой селективностью }
В дополнение к уже рассмотренным методам регуляризации,
предложены два новых класса априорных распределений направляющего вектора, наделяющих обобщенный критерий обучения (\ref{criterion}) свойством автоматического отбора признаков.
Метод релевантных признаков с регулируемой селективностью использует естественное обобщение классического метода опорных векторов за счет введения в его вероятностную постановку дополнительного предположения о том, что независимые компоненты направляющего вектора $a=(\alpha_1, ..., \alpha_n)$ распределены, как и прежде, по нормальному закону с нулевыми математическими ожиданиями, но с разными неизвестными случайными дисперсиями $r_i \geq 0, i = 1,...,n$ характеризующимися некоторым априорным распределением, и подлежащими оцениванию в процессе обучения по байесовскому принципу вместе с параметрами разделяющей гиперплоскости $(a,b)$. 
В данном методе оптимизациояная задача обучения эквивалентна
критерию [3,15]:
$$\left\{\begin{matrix}
	J_{RFM}(a, b, r, \delta |C,\mu) = \sum_{i=1}^{n}[(\frac{1}{r_i})(\alpha_i^2 +(\frac{1}{\mu}))+((\frac{1}{\mu})+1+\mu)\ln{r_i}] +\\ +C\sum_{j=1}^{N}(\delta_j)->min(a,b,r,\delta)
	\\
	y_i(\sum_{i=1}^{n}(a_ix_{ij})+b)\geq1-\delta_j, \delta_j \geq 0, j= 1,...,N, C=2c
\end{matrix}\right.$$

Итерационная процедура обучения обычно сходится за 10-15 шагов, а сам алгоритм
демонстрирует способность подавлять неинформативные признаки за счет выбора очень
маленьких весов в решающем правиле.
\\

\subsubsection{Метод опорных признаков с регулируемой селективностью}
Данный метод базируется на другом
предположении об априорном распределении $\varPsi(a)$ независимых компонент направляющего вектора а $a=(\alpha_1, ..., \alpha_n)$. Вид априорной плотности $\varPsi(a)$ представляет собой комбинацию распределения Лапласа при значениях нормы компонент и нормального распределения при больших значениях $|\alpha_{i}>\geq\mu|$ 
В данном методе мы имеем следующий вид оптимизационной задачи обучения [3,17]:
$$\left\{\begin{matrix}
	J_{SFM}(a, b, \delta |C,\mu) = 2\mu\sum_{|\alpha_{i}|\leq\mu}|\alpha_{i}| + \sum_{|\alpha_{i}|\leq\mu}(\alpha_{i}^2 + \mu^2)+ C\sum_{j=1}^{N}(\delta_j)->min(a,b,\delta)
	\\
	y_i(\sum_{i=1}^{n}(a_ix_{ij})+b)\geq1-\delta_j, \delta_j \geq 0, j= 1,...,N, C=2c
\end{matrix}\right.$$
Задача является задачей выпуклого программирования в пространстве $R^n \times R^{N+1}$, поскольку каждое слагаемое целевой функций выпукло в $R^n$, а система ограничений для заданной обучающей выборки образует выпуклый многогранник.
\\

Оптимальные ненулевые значения компонент направляющего вектора, как и в классическом методе опорных векторов, выражаются в виде линейной комбинации опорных объектов обучающей выборки, но, в отличие от SVM, решение задачи явным образом указывает множество строго нулевых компонент, а, следовательно, множество неактивных или неопорных признаков, т.е. признаков, не участвующих
в принятии решения о классе нового объекта. По этой причине, по аналогии с методом
опорных векторов, предложенный метод назван  методом опорных признаков
с регулируемой селективностью или \textbf{Selective Support Feature Support Vector Machine}


\end{document}


