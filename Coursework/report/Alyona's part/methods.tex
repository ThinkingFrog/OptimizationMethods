\documentclass[12pt,a4paper]{article}
\begin{document}
	Рассматривается задача математического программирования
	\begin{equation}\label{eq:0}
		f(x) \longrightarrow min, \quad F(x) = 0,\quad G(x) \leq 0,
	\end{equation}
где $f:R^n \longrightarrow R$ -гладкая функция, а $F:R^n \longleftrightarrow R^l$ и $G:R^n \longrightarrow R^m$ - гладкие отображения.
\subsection{Метод внутренней точки (IP)}
Метод внутренней точки - это метод позволяющий решать задачи выпуклой оптимизации с условиями, заданными в виде неравенств.
	
Согласно методам внутренней точки, исходную для поиска точку можно выбирать только внутри допустимой области.
	
Выбор начальной точки поиска осуществляется в зависимости от формулировки задачи. При отсутствии ограничений или их преобразовании к функциям штрафа с внешней точкой начальная точка выбирается произвольно. При наличии ограничений или их преобразовании к функциям штрафа с внутренней точкой начальная точка выбирается внутри допустимой области.

При этом множество точек делится на допустимые и недопустимые в зависимости от ограничений. В свою очередь множество допустимых точек в зависимости от ограничений также делится на граничные и внутренние
	
\subsubsection{Прямо-двойственный метод внутренней точки}
Прямо-двойственный метод внутренней точки) оптимизирует прямые и двойственные переменные $(x, \lambda, \mu)$ путем решения линеаризованной возмущенной системы Куна-Таккера:
	\begin{equation}
		\left[
		\begin{matrix}
			\bigtriangledown^2f_0(x) + \sum_{i=1}^{m}	\bigtriangledown^2f_i(x)&  	\bigtriangledown f(x)^T & A^T\\ 
			diag(\lambda)\bigtriangledown f(x) & diag(f(x))  & O\\
			A & O & O
		\end{matrix}
		\right] 
		\left[
		\begin{matrix}
			d_x \\
			d_\lambda \\
			d_\mu \\
		\end{matrix}
		\right] = - 
		\left[
		\begin{matrix}
			\bigtriangledown f_0(x) + 	\bigtriangledown f(x)^T\lambda + A^T\mu \\  	
			diag(\lambda)f(x) + \frac{1}{\tau}\epsilon\\
			Ax - b
		\end{matrix}
		\right] =
	\end{equation}
	\begin{equation}
		 = - \left[
		\begin{matrix}
			r_{dual}(x, \lambda, \mu) \\
			r_{center}(x, \lambda, \mu) \\
			r_{primal}(x, \lambda, \mu) \\
		\end{matrix}
		\right] = -r(x, \lambda, \mu).
	\end{equation}

Здесь через $diag(\lambda)$ обозначена диагональная матрица, в которой на диагонали стоит вектор $\lambda$, через $f(x)$ – вектор $[f_1(x), . . . , f_m(x)]^T$, через $\bigtriangledown f(x)$ – матрица производных, в которой в позиции $(ij)$ стоит $\frac{\partial}{\partial x_i}f_i(x)$, а через $e$ – вектор из единиц.

В данном случае величина $||r(x, \lambda, \mu)||$ отражает прогресс итерационного процесса, и на каждой итерации вдоль найденного направления $(d_x, d_\lambda, d_\mu)$ решается задача одномерной минимизации $||r||$. При ее решении с помощью стратегии backtracking значение $\alpha$ уменьшается до выполнения условия
	\begin{equation}
		||r(x+\alpha d_{x}, \lambda+\alpha d_{\lambda}, \mu+\alpha d_{\mu})|| < (1-\alpha  \rho)||(x,\lambda,\mu)||,
	\end{equation}		
где $\rho$ – параметр, задаваемый пользователем. Таким образом, получаем следующую общую схему прямодвойственного метода внутренней точки:
	\begin{enumerate}
		\item Выбирается строго допустимая точка $x$, положительный вектор $\lambda$ и произвольный вектор $\mu$. Также выбирается точность оптимизации $\epsilon$, $\epsilon_{feas}$, начальное значение $\tau=\tau_0$, мультипликатор $\upsilon$> 1 и параметры
		стратегии backtracking;
		\item Находится решение $(d_x, d_\lambda, d_\mu)$ СЛАУ (14) для текущего значения $\tau$;
		\item Решается задача одномерной минимизации $	||r(x + \alpha d_x, \lambda +\alpha d_\lambda, \mu + \alpha d_\mu)|| \longrightarrow min, \alpha \geq 0$	с помощью backtracking;
		\item  Если для нового набора $(x, \lambda, \mu)$ выполнено условие $||r_{dual} (x, \lambda, \mu) < \epsilon_{feas}||$, $||r_{primal} (x, \lambda, \mu) < \epsilon_{feas}||$ и $-\lambda^Tf(x)$, то алгоритм заканчивает работу;
		\item Значение $\tau$ устанавливается как $min(\frac{m}{\epsilon}, \tau(\frac{m}{-\lambda^{T}f(x)}))$
		, переход в шагу 2.
	\end{enumerate}
	
Для прямо-двойственного метода внутренней точки доказана теоретическая сходимость за \textbf{$O(\sqrt{n}\log(\frac{1}{\epsilon}))$}
итераций. Эта теоретическая оценка худшего числа итераций является наилучшей на сегодняшний день оценкой
среди всех методов решения задач условной оптимизации. На практике метод внутренней точки сходится за
константное число итераций, практически не зависящее от размерности задачи (обычно 30–40 итераций). Это
обстоятельство делает метод особенно привлекательным для использования в случае данных большого объема.
Метод внутренней точки основан на итерациях Ньютона, обладающих квадратичной скоростью сходимости.
Поэтому метод внутренней точки позволяет находить решение задачи квадратичного программирования, в том числе, для высокой точности $\epsilon$.

Этот момент является актуальным, например, для разреженных линейных моделей классификации/регрессии,
в которых обнуление максимального количества компонент позволяет получать компактные и легко интерпретируемые решающие правила, предъявляющие минимальные требования к вычисляемому набору признаков,
необходимому для принятия решений

\subsection{Последовательное квадратичное программирование (SQP)}	
Последовательное квадратичное программирование (англ. Sequential quadratic\\programming (SQP)) — один из наиболее распространённых и эффективных оптимизационных алгоритмов общего назначения, основной идеей которого является последовательное решение задач квадратичного программирования, аппроксимирующих данную задачу оптимизации. 
	
Для оптимизационных задач без ограничений алгоритм SQP преобразуется в метод Ньютона поиска точки, в которой градиент целевой функции обращается в ноль. Для решения исходной задачи с ограничениями-равенствами метод SQP преобразуется в специальную реализацию ньютоновских методов решения системы Лагранжа.
	
SQP методы генерируют траекторию $\left\lbrace x^k \right\rbrace \subset R^n$ следующим образом: по текущему приближению $x^k$  очередное приближение $x^{k+1}$ ищется как локальное решение (или как стационарная точка) задачи квадратичного программирования
	\begin{equation}\label{eq:1}
		\left\langle f'(x^k), x-x^k \right\rangle  + \frac{1}{2}\left\langle H_k(x-x^k), x-x^k \right\rangle \longrightarrow min,
	\end{equation}


	\begin{equation}\label{eq:2}
		F(x^k)+F(x^k)(x-x^k)=0, G(x^k)+G'(x^k)(x-x^k) \leq 0,
	\end{equation}

где $H_k$ - симметрическая $n \times n$-матрица, которая в некотором смысле аппроксимирует $\frac{\partial^2 L}{\partial x^2}(\bar{x}, \bar{\lambda}, \bar{\mu})$ при $k\longrightarrow\infty$\\ Например, можно полагать
	\begin{equation}
	H_k = \frac{\partial^2 L}{\partial x^2}(x, \lambda, \mu)
	\end{equation}
если параллельно с прямой траекторией ${x^k}$ генерировать двойственную траекторию ${(\lambda^k, \mu^k)}$,
	например, следующим образом: по текущим $\lambda^k, \mu^k$ очередная пара $(\lambda^{k+1}, \mu^{k+1})$ определяется как пара множителей Лагранжа, отвечающих стационарной точке $x^{k+1}$ задачи \ref{eq:1}, \ref{eq:2}. 
	\subsubsection{Последовательное линейно-квадратичное программирование (SLQP)}

Последовательное линейно-квадратичное программирование ( SLQP ) - это итерационный метод для задач нелинейной оптимизации, в котором целевая функция и ограничения дважды непрерывно дифференцируемы . Подобно последовательному квадратичному программированию (SQP), SLQP выполняется путем решения последовательности подзадач оптимизации. 

Разница между двумя подходами заключается в том, что:
\begin{itemize}
	\item в SQP каждая подзадача представляет собой квадратичную программу с квадратичной моделью цели, подлежащей линеаризации ограничений.
	\item в SLQP на каждом шаге решаются две подзадачи: линейная программа (LP), используемая для определения активного набора , за которой следует квадратичная программа с ограничениями равенства (EQP), используемая для вычисления общего шага
\end{itemize}

Эта декомпозиция делает SLQP подходящим для крупномасштабных задач оптимизации, для которых доступны эффективные решатели LP и EQP, причем эти задачи легче масштабировать, чем полноценные квадратичные программы.

\paragraph{Фаза LP}
В LP-фазе SLQP решается следующая линейная программа:
\begin{align*}
	min f(x_k) + \bigtriangledown f(x_k)^Td \\
	s.t. \quad b(x_k) + \bigtriangledown b(x_k)^Td \geq 0 \\
	c(x_k) + \bigtriangledown c(x_k)^Td=0
\end{align*}

Пусть $\mathcal{A}_{k}$ обозначает активный набор в оптимуме $ d_{\text{LP}}^{*}$ этой проблемы, то есть набор ограничений, которые равны нулю в $d_{\text{LP}}^{*}$. Обозначим через $b_{{\cal{A}}_{k}}$ и $c_{{\cal{A}}_{k}}$ подвекторы $b$ и $c$, соответствующие элементам ${\cal{A}}_{k}$.

\paragraph{Фаза EQP}
На этапе EQP SLQP направление поиска $d_{k}$ шага получается путем решения следующей квадратичной программы:
\begin{align*}
	min f(x_k) + \bigtriangledown f(x_k)^Tdc+ \frac{1}{2}d^T \bigtriangledown_{xx}^{2}{\mathcal{L}}(x_k, \lambda_k, \sigma_k)d\\
	s.t. \quad b_{\mathcal{A}_k}(x_k) + \bigtriangledown  b_{\mathcal{A}_k}(x_k)^Td = 0\\
	  \quad c_{\mathcal{A}_k}(x_k) + \bigtriangledown  c_{\mathcal{A}_k}(x_k)^Td=0
\end{align*}

Стоит обратить внимание, что член $f(x_{k})$ в приведенных выше целевых функциях может не учитываться для задач минимизации, поскольку он постоянен.

\subsubsection{Условия локальной сходимости}
Локальное поведение методов ньютоновского типа для задачи \ref{eq:0} обычно исследуют предполагая выполнение в искомом локальном решении $х$ этой задачи тех или иных условий регулярности ограничений и достаточных условий второго порядка.
	
Важнейшим условием регулярности ограничений является \textbf{условие Мангасариана-Фромовица MFCQ (от английского Mangasarian-Fromovitz constraint\\qualification)}:
	$$
		rank F(\bar{x}) = l, \exists\bar\xi \in kerF'(\bar{x} : G'_{l(\bar{x})}(\bar{x})\bar{\xi} \leq 0 
	$$

Выполнение MFCQ в стационарной точке $\bar{x}$ задачи \ref{eq:0} равносильно ограниченности полиэдра. Единственности множителей Лагранжа это условие, вообще говоря, не гарантирует. Комбинация MFCQ и требование единственности отвечающих $\bar{x}$ множителей Лагранжа $\bar{\lambda}$ и $\bar{\mu}$ называется строгим условием регулярности Мангасариана-Фромовица SMFCQ (от английского Strict Mangasarian-Fromovitz constraint\\qualification). Это условие можно записать в виде:
	\begin{equation}
		\begin{pmatrix}
		F'(x)\\
			G'_{l_{+}(\bar{x}, \bar{\mu})}(\bar{x})
		\end{pmatrix} = l + |I_{+}(\bar{x}, \bar{\mu})|,
	\exists \xi \in kerF'(\bar{x}): 	G'_{l_{+}(\bar{x}, \bar{\mu})}(\bar{x})\bar{xi} = 0, 	G'_{0_{+}(\bar{x}, \bar{\mu})}(\bar{x})\bar{xi} < 0,
	\end{equation}
где 
$I_+(\bar{x},\bar{xi}) = {i \in I(\bar{x})|\bar{\mu}_i > 0}$, 
$ I_0(\bar{x},\bar{xi}) = I(\bar{x}) \ I_+(\bar{x},\bar{xi})$	

Для $(\bar{\lambda}, \bar{\mu})$ достаточное условие второго порядка SOSC (от английского Second-order sufficient condition) имеет вид
	\begin{equation}
		\frac{\partial^2 L}{\partial x^2}(\bar{x}, \bar{\lambda}, \bar{\mu}\xi, \xi) > 0 \forall\xi \in C(\bar{x})	
	\end{equation}	 
	где $C(\bar{x}) = {\xi \in F'(\bar{x})|G'_{I(\bar{x})}(\bar{x})\xi \leq 0)}, <f'(\bar{x}), \xi\leq 0>$ есть критический конус задачи \ref{eq:0} в точке $\bar{x}$.

Локальная сходимость методов SQP и SQLP со сверхлинейной скоростью может быть доказана при выполнении SMFCQ и SOSC. Этот результат был получен
в [7]. При этом предполагается, что в качестве очередного прямодвойственного приближения $(x^{k+1},\lambda^{k+1}, \mu^{k+1})$ берется ближайшее (или, во всяком случае, достаточно
близкое) к $(x^{k},\lambda^{k}, \mu^{k})$ решение системы ККТ задачи \ref{eq:1}, \ref{eq:2}.	

\subsection{Последовательная минимальная Оптимизация (SMO)}
Sequential minimal optimization (SMO) - алгоритм для решения задачи квадратичного программирования (QP).  SMO широко используется для обучения опорных векторных машин. Публикация алгоритма SMO в 1998 году вызвала большой ажиотаж в сообществе SVM, поскольку ранее доступные методы обучения SVM были намного сложнее и требовали дорогостоящих сторонних решателей QP.
	
SMO - итерационный алгоритм для решения задачи оптимизации:
	\begin{align*}
		\max_{\alpha}\sum_{i=1}^{n}\alpha_{i}-{\frac{1}{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}y_{i}y_{j}K(x_{i},x_{j})\alpha_{i}\alpha_{j},\\	
		 \mbox{при условии:} \qquad
		0\leq \alpha_{i}\leq C, \quad \mbox{для}\quad i=1,2,\ldots ,n,	\\
		\sum_{i=1}^{n}y_{i}\alpha_{i}=0
	\end{align*}\label{eq:smo}
где $C$ - гиперпараметр SVM, $K(x_i, x_j)$ - функция ядра, а переменные $\alpha_{i}$ являются множителями Лагранжа.\\
	
SMO разбивает проблему (\ref{eq:smo}) на серию минимально возможных подзадач, которые затем решаются аналитически. Из-за ограничения линейного равенства, включающего множители Лагранжа $\alpha_{i}$, наименьшая возможная проблема включает два таких множителя. Тогда для любых двух множителей $\alpha_{1}$ и $\alpha_{2}$ , ограничения сокращаются до:
	
$$0 \leq \alpha_{1}, \alpha_{2} \leq C,$$
$$ y_{1}\alpha_{1} + y_{2}\alpha_{2} = k,$$ 
	и эта сокращенная задача может быть решена аналитически: нужно найти минимум одномерной квадратичной функции. $k$ - отрицательное значение суммы остальных членов в ограничении равенства, которое фиксируется на каждой итерации.
	
Алгоритм работает следующим образом:
	\begin{itemize}
		\item Найти множитель Лагранжа $\alpha_{1}$, который нарушает Условия Каруша — Куна — Таккера (KKT) для задачи оптимизации.
		\item Выбрать второй множитель $\alpha_{2}$ и оптимизировать пару $(\alpha_{1}, \alpha_{2})$.
		\item Повторить шаги 1 и 2 до сходимости.
	\end{itemize}	
	
Когда все множители Лагранжа удовлетворяют условиям KKT (в пределах заданного пользователем допуска) проблема решена. Хотя этот алгоритм гарантирует сходимость, эвристика используется для выбора пары множителей, чтобы ускорить скорость сходимости. Это очень важно для больших наборов данных, поскольку существует $n(n-1)/2$ возможных вариантов для $\alpha_{i}$ и $\alpha_{j}$.
\end{document}
