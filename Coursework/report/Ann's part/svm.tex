\documentclass[12pt,a4paper]{article}

\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{indentfirst}
\usepackage{misccorr}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage[left=20mm,right=10mm, top=20mm,bottom=20mm,bindingoffset=0mm]{geometry}

\setlength{\parskip}{6pt}
\DeclareGraphicsExtensions{.png}

\begin{document}
\section{Классический метод машины опорных векторов}
\subsection{Краткое описание метода}
\textbf{Метод опорный векторов (SVM)} разработан в 60-е годы коллективом советских математиков под руководством В.Н.Вапника и рассчитан на классифицирование объектов по двум классам.

Пусть имеется обучающая выборка $G^l,|G^l|=n$, заданная множеством пар прецедентов $(\bar{x}_i,y_i),i=\overline{1,n},\bar{x}_i\in\mathbb{R}^m,y_i\in\{-1,+1\}$. Множество $F_{SVM}$, из которого выбираются решающие функции по методу опорных векторов, образовано функциями вида:
\begin{equation}
    f(\bar{x})=sign(<\bar{w},\bar{x}>+w_0),
\end{equation}
где $<,>$ - скалярное произведение векторов, $\bar{w}$ - ортонормированный вектор к разделяющей классы гиперплоскости, $w_0$ - вспомогательный параметр (сдвиг гиперплоскости).

Так как любая гиперплоскость может быть задана в виде $<\bar{w},\bar{x}>+w_0=0$, то объекты с $f(\bar{x})\leq-1$ попадут в один класс, а объекты с $f(\bar{x})\geq+1$ - в другой.

\textit{Базовая идея метода:} найти такие $\bar{w},w_0$, которые максимизируют расстояние между классами, что приводит к более уверенной классификации объектов.

То есть условие $-1<<\bar{w},\bar{x}>+w_0<+1$ задает полосу, разделяющую классы. При этом ни одна точка из множества $X^l$ не должна лежать внутри полосы, а границами полосы являются две параллельные гиперплоскости, проходящие через точки (объекты), ближайшие к разделяющей гиперплоскости, которая находится по середине данной полосы. И объекты, через которые проходят границы полосы, называются \textit{опорными векторами}.

Проблема нахождения максимума расстояния сводится к нахождению минимума $||\bar{w}||^2$, которая является стандартной задачей квадратичного программирования:
\begin{equation}
    \left\{
    \begin{array}{ll}
        ||\bar{w}||^2\rightarrow\min\\
        y_i<\bar{w},\bar{x}_i+w_0>\geq1>\\
    \end{array}
    \right.
\end{equation}
и решается методом множителей Лагранжа.

Задача квадратичного программирования, содержащая только двойственные переменные метода множителей Лагранжа $\lambda_i$, имеет вид:
\begin{equation}
    \left\{
    \begin{array}{ll}
        -\sum_{i=1}^n{\lambda_i}+\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n{\lambda_i\lambda_jy_iy_j<\bar{x}_i,\bar{x}_j>}\rightarrow\min\\
        \sum_{i=1}^n{\lambda_iy_i}=0\\
        \lambda_i\geq0\\
    \end{array}
    \right.
\end{equation}

В результате рещающая функция приобретает вид:
\begin{equation}
    f(\bar{x})=sign(\sum_{i=1}^n{\lambda_iy_i<\bar{x}_i,\bar{x}>+w_0)},
\end{equation}
где параметр $w_0=med\{<\bar{w},\bar{x}_i>-y_i\}, \lambda_i\not=0$.

Вышеуказанные рассуждения справедливы для линейно разделимой обучающей выборки. Но на практике встречаются случае линейной неразделимости и решающей функции позволяют допускать ошибки на обучающей выборке, но эти ошибки минимизируют и используют управляющую константу $C$ как компромисс между максимизацией ширины разделяющей полосы и минимизацией суммарной ошибки. Тогда вводят ограничение сверху $0\leq\lambda_i\leq{C}$ и такой алгоритм называют SVM с "мягким зазором" (soft-margin SVM), иначе имеется "жесткий" зазор.

Если признаки $x_i$ заданы в виде функции $\theta(x_i)$, то решающая функция строится аналогично. Тогда функция $K(u,v)=<\theta(u),\theta(v)>$ - \textit{ядро}, если она симметрична и положительно определена. Для решения практических задач классификации изображений по атрибуту "пол" используют RFB-ядро вида:
\begin{equation}
    K(\bar{x}_i,\bar{x})=\exp{(-\gamma||\bar{x}_i-\bar{x}||^2)},
\end{equation}
вычисляющее оценку близости вектора $\bar{x}$ к опорному вектору $\bar{x}_i$, где $\gamma$ - некоторый параметр.

\subsection{Множественная классификация по атрибуту}
Классификация по атрибуту "раса" является типичной задачей множественной классификации. Для решения такой задачи используется подход \textit{"один против всех"}, реализующий сведение задачи множественной классификации к последовательному применению бинарных классификаторов. В рамках данного подхода строится бинарное дерево решающих функций $f\in{F}$, каждая из которых выделяет только один класс объектов.

В случае классификации по атрибуту "раса" на первом шаге объекты разделяются решающей фукцией $f_1$ на два класса: "европеоиды" и "все остальные". Если объект не попал в класс "европеоиды", то на втором шаге другая решающая функция $f_2$ производит разделение на класс "монголоиды" и "все остальные" ("негроиды").

Данный подход позволяет использовать большую часть разработок в области бинарной классификации для решения задач множественной классификации.
\end{document}
